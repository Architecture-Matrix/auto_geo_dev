# -*- coding: utf-8 -*-
"""
æ–‡ç« æ”¶é›†é€‚é…å™¨åŸºç±»
ç”¨é€‚é…å™¨æ¨¡å¼å®žçŽ°å„å¹³å°æ”¶é›†ï¼Œéµå¾ªå¼€é—­åŽŸåˆ™ï¼
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from playwright.async_api import Page, BrowserContext
from loguru import logger


@dataclass
class CollectedArticle:
    """æ”¶é›†åˆ°çš„æ–‡ç« æ•°æ®ç»“æž„"""
    title: str
    url: str
    content: str
    likes: int = 0
    reads: int = 0
    comments: int = 0
    author: str = ""
    platform: str = ""
    publish_time: str = ""


class BaseCollector(ABC):
    """
    åŸºç¡€æ–‡ç« æ”¶é›†é€‚é…å™¨
    æ³¨æ„ï¼šæ‰€æœ‰å¹³å°æ”¶é›†å™¨éƒ½è¦ç»§æ‰¿è¿™ä¸ªç±»ï¼
    """

    def __init__(self, platform_id: str, config: Dict[str, Any]):
        self.platform_id = platform_id
        self.config = config
        self.name = config.get("name", platform_id)
        self.search_url = config.get("search_url", "")
        self.min_likes = config.get("min_likes", 100)
        self.min_reads = config.get("min_reads", 1000)

    @abstractmethod
    async def search(self, page: Page, keyword: str) -> List[Dict[str, Any]]:
        """
        æœç´¢å…³é”®è¯ç›¸å…³æ–‡ç« 

        Args:
            page: Playwright Pageå¯¹è±¡
            keyword: æœç´¢å…³é”®è¯

        Returns:
            æœç´¢ç»“æžœåˆ—è¡¨ï¼š[{title, url, likes, reads, ...}, ...]
        """
        pass

    @abstractmethod
    async def extract_content(self, page: Page, url: str) -> Optional[str]:
        """
        æå–æ–‡ç« æ­£æ–‡å†…å®¹

        Args:
            page: Playwright Pageå¯¹è±¡
            url: æ–‡ç« URL

        Returns:
            æ–‡ç« æ­£æ–‡å†…å®¹
        """
        pass

    async def collect(self, page: Page, keyword: str) -> List[CollectedArticle]:
        """
        æ”¶é›†çˆ†ç«æ–‡ç« ï¼ˆä¸»æµç¨‹ï¼‰

        Args:
            page: Playwright Pageå¯¹è±¡
            keyword: æœç´¢å…³é”®è¯

        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ–‡ç« åˆ—è¡¨
        """
        try:
            # 1. æœç´¢æ–‡ç« 
            search_results = await self.search(page, keyword)
            logger.info(f"[{self.name}] æœç´¢åˆ° {len(search_results)} ç¯‡æ–‡ç« ")

            # 2. ç­›é€‰çˆ†ç«æ–‡ç« 
            trending_articles = self._filter_trending(search_results)
            logger.info(f"[{self.name}] ç­›é€‰å‡º {len(trending_articles)} ç¯‡çˆ†ç«æ–‡ç« ")

            # 3. æå–æ­£æ–‡å†…å®¹
            collected = []
            for article in trending_articles:
                content = await self.extract_content(page, article["url"])
                if content:
                    collected.append(CollectedArticle(
                        title=article.get("title", ""),
                        url=article.get("url", ""),
                        content=content,
                        likes=article.get("likes", 0),
                        reads=article.get("reads", 0),
                        comments=article.get("comments", 0),
                        author=article.get("author", ""),
                        platform=self.platform_id,
                        publish_time=article.get("publish_time", "")
                    ))

            return collected

        except Exception as e:
            logger.error(f"[{self.name}] æ”¶é›†æ–‡ç« å¤±è´¥: {e}")
            return []

    def _filter_trending(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ç­›é€‰çˆ†ç«æ–‡ç« 

        ç­›é€‰é€»è¾‘ï¼šç‚¹èµžæ•° > min_likes æˆ– é˜…è¯»é‡ > min_reads
        """
        trending = []
        for article in articles:
            likes = article.get("likes", 0)
            reads = article.get("reads", 0)

            if likes > self.min_likes or reads > self.min_reads:
                trending.append(article)
                logger.debug(f"[{self.name}] çˆ†ç«: {article.get('title', '')[:30]}... "
                           f"(ðŸ‘{likes}, ðŸ‘{reads})")

        return trending

    async def wait_for_selector(self, page: Page, selector: str, timeout: int = 10000) -> bool:
        """ç­‰å¾…é€‰æ‹©å™¨å‡ºçŽ°"""
        try:
            await page.wait_for_selector(selector, timeout=timeout)
            return True
        except Exception as e:
            logger.warning(f"ç­‰å¾…é€‰æ‹©å™¨è¶…æ—¶: {selector}, {e}")
            return False

    async def navigate_to_search(self, page: Page, keyword: str) -> bool:
        """å¯¼èˆªåˆ°æœç´¢é¡µé¢"""
        try:
            search_url = self.search_url.format(keyword=keyword)
            await page.goto(search_url, wait_until="networkidle")
            logger.info(f"[{self.name}] å·²å¯¼èˆªåˆ°æœç´¢é¡µ: {keyword}")
            return True
        except Exception as e:
            logger.error(f"[{self.name}] å¯¼èˆªæœç´¢é¡µå¤±è´¥: {e}")
            return False


class CollectorRegistry:
    """
    æ”¶é›†å™¨æ³¨å†Œè¡¨
    ç”¨è¿™ä¸ªæ¥ç®¡ç†æ‰€æœ‰å¹³å°çš„æ”¶é›†å™¨ï¼
    """

    def __init__(self):
        self._collectors: Dict[str, BaseCollector] = {}

    def register(self, platform_id: str, collector: BaseCollector):
        """æ³¨å†Œæ”¶é›†å™¨"""
        self._collectors[platform_id] = collector
        logger.info(f"æ”¶é›†å™¨å·²æ³¨å†Œ: {platform_id}")

    def get(self, platform_id: str) -> Optional[BaseCollector]:
        """èŽ·å–æ”¶é›†å™¨"""
        return self._collectors.get(platform_id)

    def list_all(self) -> Dict[str, BaseCollector]:
        """åˆ—å‡ºæ‰€æœ‰æ”¶é›†å™¨"""
        return self._collectors.copy()


# å…¨å±€æ³¨å†Œè¡¨
collector_registry = CollectorRegistry()


def get_collector(platform_id: str) -> Optional[BaseCollector]:
    """èŽ·å–å¹³å°æ”¶é›†å™¨"""
    return collector_registry.get(platform_id)


def list_collectors() -> Dict[str, BaseCollector]:
    """åˆ—å‡ºæ‰€æœ‰æ”¶é›†å™¨"""
    return collector_registry.list_all()
