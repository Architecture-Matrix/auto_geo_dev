# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–è°ƒåº¦æœåŠ¡ - å·¥ä¸šé²æ£’åŠ å›ºç‰ˆ
è´Ÿè´£ï¼šå®šæ—¶æ‰«æå¾…å‘å¸ƒæ–‡ç« ã€è‡ªåŠ¨è§¦å‘æ”¶å½•æ£€æµ‹ã€å¤±è´¥é‡è¯•ã€åŠ¨æ€ä»»åŠ¡åŠ è½½
é‡æ„ç‚¹ï¼š
1. å¼•å…¥ Semaphore æ§åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º 3
2. æ·»åŠ æ‰§è¡Œæ—¥å¿—è®°å½• (SchedulerExecutionLog)
3. ä»»åŠ¡è‡ªæ„ˆæœºåˆ¶ (cleanup_stuck_tasks)
4. æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
5. æ‰§è¡Œå®ˆå«æ¨¡å¼ (wrap_execution)
"""

import asyncio
import random
import traceback
from typing import Optional, Dict, Any, List, Callable
from datetime import datetime, timedelta
from functools import wraps
from loguru import logger
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from sqlalchemy.orm import Session

# å°è¯•å¯¼å…¥æ—¶åŒºï¼Œé˜²æ­¢ç¯å¢ƒç¼ºå¤±æŠ¥é”™
try:
    from pytz import timezone
except ImportError:
    timezone = None

from backend.services.geo_article_service import GeoArticleService
from backend.database.models import (
    ScheduledTask, GeoArticle, Project, Keyword,
    SchedulerExecutionLog
)

# ğŸŒŸ ç»Ÿä¸€æ—¥å¿—ç»‘å®š
log = logger.bind(module="è°ƒåº¦ä¸­å¿ƒ")

# ==================== æ‰§è¡Œå®ˆå«è£…é¥°å™¨ ====================

def wrap_execution(task_key: str):
    """
    æ‰§è¡Œå®ˆå«è£…é¥°å™¨
    è´Ÿè´£è®°å½•æ‰§è¡Œå¼€å§‹/ç»“æŸ/é”™è¯¯å †æ ˆåˆ° SchedulerExecutionLog
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            # æå– article_id
            article_id = kwargs.get('article_id') or (args[0] if args else None)

            if not self.db_factory:
                return await func(self, *args, **kwargs)

            # è·å–æ‰§è¡Œæ—¥å¿—è®°å½•å‡½æ•°
            async def log_execution(status: str, error_stack: str = None, result_summary: str = None):
                db = self.db_factory()
                try:
                    log_entry = db.query(SchedulerExecutionLog).filter(
                        SchedulerExecutionLog.task_key == task_key,
                        SchedulerExecutionLog.article_id == article_id,
                        SchedulerExecutionLog.finished_at == None
                    ).first()

                    if log_entry:
                        log_entry.finished_at = datetime.now()
                        log_entry.duration_ms = int((log_entry.finished_at - log_entry.started_at).total_seconds() * 1000)
                        log_entry.status = status
                        if error_stack:
                            log_entry.error_stack_trace = error_stack
                        if result_summary:
                            log_entry.result_summary = result_summary
                        db.commit()
                except Exception as e:
                    logger.error(f"è®°å½•æ‰§è¡Œæ—¥å¿—å¤±è´¥: {e}")
                finally:
                    db.close()

            # åˆ›å»ºè¿è¡Œä¸­æ—¥å¿—
            db = self.db_factory()
            try:
                execution_log = SchedulerExecutionLog(
                    task_key=task_key,
                    article_id=article_id,
                    started_at=datetime.now(),
                    status="running"
                )
                db.add(execution_log)
                db.commit()
                db.refresh(execution_log)
            except Exception as e:
                logger.error(f"åˆ›å»ºæ‰§è¡Œæ—¥å¿—å¤±è´¥: {e}")
            finally:
                db.close()

            # æ‰§è¡Œè¢«è£…é¥°å‡½æ•°
            try:
                result = await func(self, *args, **kwargs)
                # è®°å½•æˆåŠŸ
                result_summary = f"å‘å¸ƒæˆåŠŸï¼Œè¿”å›: {result}"
                await log_execution("success", result_summary=result_summary)
                return result
            except Exception as e:
                # è®°å½•å¤±è´¥
                error_stack = traceback.format_exc()
                await log_execution("failed", error_stack=error_stack, result_summary=f"æ‰§è¡Œå¼‚å¸¸: {str(e)}")
                logger.error(f"æ‰§è¡Œå¤±è´¥: {e}\n{error_stack}")
                raise
        return wrapper
    return decorator


# æŒ‡æ•°é€€é¿æ—¶é—´é…ç½®ï¼ˆåˆ†é’Ÿï¼‰
RETRY_DELAYS = [5, 30, 120]  # ç¬¬1æ¬¡5åˆ†é’Ÿï¼Œç¬¬2æ¬¡30åˆ†é’Ÿï¼Œç¬¬3æ¬¡2å°æ—¶


class SchedulerService:
    def __init__(self):
        tz = timezone('Asia/Shanghai') if timezone else None
        # é…ç½®è°ƒåº¦å™¨ï¼Œè®¾ç½®è¾ƒé•¿çš„è¯¯ç«å®¹å¿æ—¶é—´
        self.scheduler = AsyncIOScheduler(
            timezone=tz,
            job_defaults={
                'misfire_grace_time': 60, # ğŸŒŸ å…è®¸é”™è¿‡æ—¶é—´å60ç§’å†…é‡è¯•
                'coalesce': True,         # ç§¯å‹çš„ä»»åŠ¡åªè·‘ä¸€æ¬¡
                'max_instances': 1        # åŒä¸€ä¸ªJobåŒæ—¶åªèƒ½è·‘ä¸€ä¸ªå®ä¾‹
            }
        )
        self.db_factory = None

        # ğŸŒŸ å¹¶å‘æ§åˆ¶ï¼šæœ€å¤§åŒæ—¶æ‰§è¡Œ3ä¸ªå‘å¸ƒä»»åŠ¡
        self._publish_semaphore = asyncio.Semaphore(3)

        # ğŸŒŸ ä»»åŠ¡æ˜ å°„è¡¨
        self.task_registry = {
            "publish_task": self.check_and_publish_scheduled_articles,
            "monitor_task": self.auto_check_indexing_job
        }

    def set_db_factory(self, db_factory):
        self.db_factory = db_factory

    def init_default_tasks(self):
        """åˆå§‹åŒ–é»˜è®¤å®šæ—¶æ‰«æä»»åŠ¡"""
        if not self.db_factory: return
        db = self.db_factory()
        try:
            if db.query(ScheduledTask).count() == 0:
                defaults = [
                    ScheduledTask(
                        name="æ–‡ç« è‡ªåŠ¨å‘å¸ƒå¼•æ“",
                        task_key="publish_task",
                        cron_expression="*/1 * * * *",  # æ¯åˆ†é’Ÿæ‰«æä¸€æ¬¡
                        description="æ‰«æå¾…å‘å¸ƒæ–‡ç« å¹¶è§¦å‘æµè§ˆå™¨è‡ªåŠ¨åŒ–è„šæœ¬",
                        is_active=True
                    ),
                    ScheduledTask(
                        name="å…¨ç½‘æ”¶å½•å®æ—¶ç›‘æµ‹",
                        task_key="monitor_task",
                        cron_expression="*/5 * * * *",  # æ¯5åˆ†é’Ÿç›‘æµ‹ä¸€æ¬¡
                        description="é€šè¿‡AIæœç´¢å¼•æ“æ£€æŸ¥å·²å‘å¸ƒæ–‡ç« çš„æ”¶å½•çŠ¶æ€",
                        is_active=True
                    )
                ]
                db.add_all(defaults)
                db.commit()
                log.info("âœ… é»˜è®¤å®šæ—¶æ‰«æä»»åŠ¡åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            log.error(f"åˆå§‹åŒ–ä»»åŠ¡å¤±è´¥: {e}")
        finally:
            db.close()

    def _schedule_job(self, task: ScheduledTask):
        """å†…éƒ¨æ–¹æ³•ï¼šæ³¨å†Œ/æ›´æ–°å•ä¸ª Job"""
        func = self.task_registry.get(task.task_key)
        if not func:
            log.warning(f"âš ï¸ æœªæ‰¾åˆ°å¤„ç†å‡½æ•°: {task.task_key}")
            return

        if self.scheduler.get_job(task.task_key):
            self.scheduler.remove_job(task.task_key)

        if task.is_active:
            try:
                self.scheduler.add_job(
                    func,
                    CronTrigger.from_crontab(task.cron_expression),
                    id=task.task_key,
                    replace_existing=True,
                    misfire_grace_time=60 # ğŸŒŸ åŠ å›ºä¿æŠ¤
                )
                log.info(f"ğŸ“… ä»»åŠ¡è£…è½½æˆåŠŸ: [{task.name}] -> {task.cron_expression}")
            except Exception as e:
                log.error(f"âŒ Cron è¡¨è¾¾å¼è§£æé”™è¯¯ [{task.name}]: {e}")

    def load_jobs_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½å¹¶æ³¨å†Œæ‰€æœ‰ä»»åŠ¡"""
        if not self.db_factory: return
        db = self.db_factory()
        try:
            tasks = db.query(ScheduledTask).all()
            for t in tasks:
                self._schedule_job(t)
        finally:
            db.close()

    async def cleanup_stuck_tasks(self):
        """
        ä»»åŠ¡è‡ªæ„ˆï¼šæ¸…ç†å¡æ­»çš„ä»»åŠ¡
        å°†åœç•™åœ¨ publishing çŠ¶æ€è¶…è¿‡é˜ˆå€¼ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰çš„æ–‡ç« é‡ç½®ä¸º failed
        """
        if not self.db_factory:
            return

        db = self.db_factory()
        try:
            threshold = datetime.now() - timedelta(minutes=30)
            stuck_articles = db.query(GeoArticle).filter(
                GeoArticle.publish_status == "publishing",
                GeoArticle.updated_at < threshold
            ).all()

            if stuck_articles:
                log.warning(f"ğŸ”§ [è‡ªæ„ˆ] å‘ç° {len(stuck_articles)} ç¯‡å¡æ­»æ–‡ç« ï¼Œæ­£åœ¨é‡ç½®çŠ¶æ€...")
                for article in stuck_articles:
                    article.publish_status = "failed"
                    article.error_msg = "ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼Œè‡ªåŠ¨é‡ç½®"
                    log.warning(f"  -> æ–‡ç«  {article.id} å·²ä» publishing é‡ç½®ä¸º failed")

                    # è®°å½•è¶…æ—¶æ—¥å¿—
                    log_entry = SchedulerExecutionLog(
                        task_key="publish_task",
                        article_id=article.id,
                        started_at=article.updated_at,
                        finished_at=datetime.now(),
                        status="timeout",
                        result_summary="ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼Œè‡ªåŠ¨é‡ç½®"
                    )
                    db.add(log_entry)

                db.commit()
                log.success(f"âœ… [è‡ªæ„ˆ] å·²é‡ç½® {len(stuck_articles)} ç¯‡å¡æ­»æ–‡ç« ")
        except Exception as e:
            log.error(f"ä»»åŠ¡è‡ªæ„ˆå¤±è´¥: {e}")
        finally:
            db.close()

    def start(self):
        """å¯åŠ¨è°ƒåº¦å¼•æ“"""
        if not self.scheduler.running:
            # å¯åŠ¨ä»»åŠ¡è‡ªæ„ˆï¼ˆå¿…é¡»åœ¨åŠ è½½ä»»åŠ¡å‰æ‰§è¡Œï¼‰
            asyncio.create_task(self.cleanup_stuck_tasks())
            self.init_default_tasks()
            self.load_jobs_from_db()
            self.scheduler.start()
            log.success("ğŸš€ [Scheduler] åŠ¨æ€è°ƒåº¦å¼•æ“å·²å…¨é¢å¯åŠ¨")

    def stop(self):
        """å®‰å…¨åœæ­¢"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            log.info("ğŸ›‘ [Scheduler] è°ƒåº¦å¼•æ“å·²å®‰å…¨å…³é—­")

    def reload_task(self, task_id: int):
        """ç”¨æˆ·ä¿®æ”¹é…ç½®åï¼Œæ‰‹åŠ¨çƒ­æ›´æ–°"""
        if not self.db_factory: return
        db = self.db_factory()
        try:
            task = db.query(ScheduledTask).get(task_id)
            if task:
                self._schedule_job(task)
                return True
        finally:
            db.close()
        return False

    # ================= ğŸš€ æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ Job =================

    async def _publish_articles_internal(self, articles: List[GeoArticle]):
        """
        å†…éƒ¨æ–¹æ³•ï¼šä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘å‘å¸ƒ
        ç¡®ä¿æœ€å¤šåŒæ—¶æ‰§è¡Œ 3 ä¸ªå‘å¸ƒä»»åŠ¡
        """
        async def publish_with_semaphore(article: GeoArticle):
            async with self._publish_semaphore:
                try:
                    service = GeoArticleService(self.db_factory())
                    await service.execute_publish(article.id)
                finally:
                    # é‡Šæ”¾ db è¿æ¥
                    self.db_factory().close()

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡ï¼Œä½†é€šè¿‡ Semaphore é™åˆ¶å¹¶å‘
        tasks = [publish_with_semaphore(article) for article in articles]
        await asyncio.gather(*tasks, return_exceptions=True)

    async def check_and_publish_scheduled_articles(self):
        """
        [Job] è‡ªåŠ¨æ‰«æå¹¶å‘å¸ƒ
        å¢å¼ºåŠŸèƒ½ï¼š
        1. ä½¿ç”¨ Semaphore é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º 3
        2. æ”¯æŒæŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
        """
        if not self.db_factory: return
        db = self.db_factory()
        try:
            now = datetime.now()
            # æœç´¢ï¼šå¾…å‘å¸ƒ(scheduled) æˆ– å¤±è´¥é‡è¯•(failed ä¸” æ—¶é—´å·²åˆ°)
            pending = db.query(GeoArticle).filter(
                ((GeoArticle.publish_status == "scheduled") |
                 ((GeoArticle.publish_status == "failed") & (GeoArticle.publish_time <= now))),
                GeoArticle.retry_count < len(RETRY_DELAYS)  # æœªè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
            ).all()

            if pending:
                log.info(f"ğŸ” [å‘å¸ƒæ‰«æ] å‘ç° {len(pending)} ç¯‡å¾…å‘å¸ƒæ–‡ç« ï¼Œå‡†å¤‡è§¦å‘è„šæœ¬...")
                # ä½¿ç”¨å†…éƒ¨æ–¹æ³•è¿›è¡Œå¹¶å‘æ§åˆ¶
                await self._publish_articles_internal(pending)
        except Exception as e:
            log.error(f"å‘å¸ƒ Job è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            db.close()

    async def auto_check_indexing_job(self):
        """
        [Job] è‡ªåŠ¨ç›‘æµ‹æ”¶å½•
        """
        if not self.db_factory: return
        db = self.db_factory()
        try:
            # æœç´¢ï¼šå·²å‘å¸ƒ ä½† æœªè¢«ç¡®è®¤æ”¶å½•çš„æ–‡ç« 
            pending = db.query(GeoArticle).filter(
                GeoArticle.publish_status == "published",
                GeoArticle.index_status != "indexed"
            ).all()

            if pending:
                log.info(f"ğŸ“¡ [æ”¶å½•æ‰«æ] å‘ç° {len(pending)} ç¯‡å·²å‘å¸ƒæ–‡ç« éœ€è¦æ£€æµ‹æ•ˆæœ...")
                service = GeoArticleService(db)
                for article in pending:
                    asyncio.create_task(service.check_article_index(article.id))
        except Exception as e:
            log.error(f"ç›‘æµ‹ Job è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            db.close()

# å•ä¾‹æ¨¡å¼
_instance = SchedulerService()

def get_scheduler_service():
    return _instance